# VQ-VAE 블로그 정리

## Introduction

VQ-VAE 이전의 VAE를 활용한 논문들은 모두 Continous하게 데이터를 표현하기 위해 Distribution을 추정합니다. VAE가 등장함으로서 우리는 빠른 Sampling과 데이터의 latent vector에 대해 더 주목할 수 있다는 장점이 있었습니다. 그러나 우리가 다루는 데이터들(Image, Audio, Video 등) 대부분은 사실 이미 Digital 상에서 표현된 데이터들입니다. 

![Untitled](VQ-VAE%20%E1%84%87%E1%85%B3%E1%86%AF%E1%84%85%E1%85%A9%E1%84%80%E1%85%B3%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%203078bcb2b598424da51d64b10ec83ff0/Untitled.png)

이들은 모두 Continous하게 표현되지 않고 위 그림에서 보인는 것처럼 사실은 Discrete 한 Digital number로 구성된 Dataset을 가지고 Input으로 학습하며 Output 또한 그렇게 표현을 만듭니다. 그렇기에 해당 논문에서는 Latent vector 또한 Discrete하게 표현되어야 한다고 주장합니다. 물론 과거에 이산적인 변수를 가지고 학습을 하는 것을 다른 이들이 안 떠올리지는 않았을 것입니다. 하지만 이산적인 변수는 Backprop이 불가능하여 해당 논문에서는 이산적인 표현을 살리면서 미분이 가능하도록 하는 대체 방안도 함께 제시합니다.. 이는 구조 및 loss부분에서 자세히 설명 드리겠습니다.

따라서 저자는 VAE와 이산 표현을 결합한 새로운 생성모델인 VQ-VAE라는 새로운 생성모델을 제안합니다. 여기서 말하는 VQ란 Vector Quantization의 약자로 VAE 구조에 VQ기능이 들어간 모델로서, 기존 VAE 구조에 Vector Quantisation(VQ)를 사용하여 기존에 continous하게 latent vector를 구성하던 것을 discrete한 latent vector로 만들어 아래와 같은 구조로 변경합니다.

![Untitled](VQ-VAE%20%E1%84%87%E1%85%B3%E1%86%AF%E1%84%85%E1%85%A9%E1%84%80%E1%85%B3%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%203078bcb2b598424da51d64b10ec83ff0/Untitled%201.png)

# Model Structure

![https://user-images.githubusercontent.com/60708119/200171694-c35e6a49-f222-41e0-9f9e-9cfe8bf060cd.png](https://user-images.githubusercontent.com/60708119/200171694-c35e6a49-f222-41e0-9f9e-9cfe8bf060cd.png)

- n : batch size
- h : image height
- w : image width
- c : number of channels on the input image
- d : number of channels in the hidden state

# Processing

![https://user-images.githubusercontent.com/60708119/200171775-52cf265a-8534-4f71-97e0-24db6cc4a9cf.png](https://user-images.githubusercontent.com/60708119/200171775-52cf265a-8534-4f71-97e0-24db6cc4a9cf.png)

1. **Reshape** : 마지막 차원을 제외한 모든 차원이 하나로 결합되어 각 차원의 n×h×w 벡터를 갖습니다.
2. **Calculating distances** : n×h×w 벡터 각각에 대해 임베딩 딕셔너리의 k 벡터 각각으로부터 거리를 계산하여 (n×h×w, k) 형태의 행렬을 얻습니다.
3. **Argmin** : n×h×w 벡터 각각에 대해 사전에서 가장 가까운 k 벡터의 인덱스를 찾습니다.
4. **Index from dictionary** : n×h×w 벡터 각각에 대해 사전에서 가장 가까운 벡터를 색인화합니다.
5. **Reshape** : convert back to shape (n, h, w ,d)
6. **Copying gradients** : If you followed up till now you'd realize that it's not possible to train this architecture through backpropagation as the gradient won't flow through argmin. Hence we try to approximate by copying the gradients from z_q back to z_e. In this way we're not actually minimizing the loss function but are still able to pass some information back for training.

# Vector Quantisation

The posterior categorical distribution q(z|x) probabilities are defined as one-hot as follows

![https://user-images.githubusercontent.com/60708119/200172372-f21bb15c-798b-4a9b-89cd-e20139ce6844.png](https://user-images.githubusercontent.com/60708119/200172372-f21bb15c-798b-4a9b-89cd-e20139ce6844.png)

- z_e(x)*ze*(*x*) is the output of the encoder network
- IN VAE, *log* *p*(*x*) can bound with ELBO, VQ-VAE proposal distribution *q*(*z*=*k*∣*x*) is deterministic
    
    log\ p(x)
    
    q(z=k|x)
    
- By defining a simple uniform prior over z we obtain a KL divergence constant and equal to log K

# Model Structure

![VQVAE1](https://user-images.githubusercontent.com/60708119/200171694-c35e6a49-f222-41e0-9f9e-9cfe8bf060cd.png)  
- n : batch size
- h : image height
- w : image width
- c : number of channels on the input image
- d : number of channels in the hidden state

# Processing
![VQVAE2](https://user-images.githubusercontent.com/60708119/200171775-52cf265a-8534-4f71-97e0-24db6cc4a9cf.png)  
1. **Reshape** : all dimensions except the last one are combined into one so that we habe n×h×w vectors each if dimensionality d
2. **Calculating distances** : for each of the n×h×w vectors we calculate distance from each of k vectors of the embedding dictionary to obtain a matrix of shape (n×h×w, k)
3. **Argmin** : for each of the n×h×w vectors we find the index of closest of the k vectors from dictionary
4. **Index from dictionary** : index the closest vector from the dictionary for each of n×h×w vectors
5. **Reshape** : convert back to shape (n, h, w ,d)
6. **Copying gradients** : If you followed up till now you'd realize that it's not possible to train this architecture through backpropagation as the gradient won't flow through argmin. Hence we try to approximate by copying the gradients from z_q back to z_e. In this way we're not actually minimizing the loss function but are still able to pass some information back for training.

# Vector Quantisation
The posterior categorical distribution q(z|x) probabilities are defined as one-hot as follows  
<img width="792" alt="VQVAE3" src="https://user-images.githubusercontent.com/60708119/200172372-f21bb15c-798b-4a9b-89cd-e20139ce6844.png">  
- $z_e(x)$ is the output of the encoder network
- IN VAE, $ log\ p(x) $ can bound with ELBO, VQ-VAE proposal distribution $q(z=k|x)$ is deterministic
- By defining a simple uniform prior over z we obtain a KL divergence constant and equal to log K

<img width="785" alt="VQVAE4" src="https://user-images.githubusercontent.com/60708119/200172375-26896a4f-a80e-446f-a17b-de7ed2c55688.png"> 

- The representation $z_e(x)$ is passed through the discretizaion bottle neck followed by mapping onto the nearest element of embedding $e$ as given in equation 1 and 2.

<img width="669" alt="VQVAE5" src="https://user-images.githubusercontent.com/60708119/200172632-d6109327-d959-4e75-90c4-91cdc40a769a.png">    

- 모델은 입력 x로부터 받아 인코더를 통해 출력 $z_e(x)$를 생성함
- Discrete latent variable z는 shared embedding space인 e로부터 가장 가까운 neighborhoodd와의 distance 계산에 의해 결정됨. 즉 가장 가까운 vector로 결정
- e인 embedding space를 codebook으로 부르며, $e\ \in\ R^{K×D}$ 이며, 여기에서 K는 discrete latent space의 size가 됨(즉, K-way categorical)
- D는 각각의 latent embedding vector인 $e_i$ 의 차원 수
- 종합적으로, K 개의 embedding vector가 존재하며, 이를 수식으로 나타내면 $e_i\ \in\ R^D,\ i\ \in\ 1, 2, ..., K$ 가 됨  

# Loss function  
> Total Loss  
  
$$ L = log\ p(x|z_q(x)) + ||sg[z_e(x)]-e||_2^2\ + \beta||z_e(x)-sg[e]||_2^2  $$  
- sg : stand for the stopgradient operator
- Decoder optimized the first loss term only
- Encoder optimizes the first and the last loss terms
- Embeddings are optimized by the middle loss term
 

##
- Since VQ-VAE assume a uniform prior for z, the KL term that usually appears in the ELBO is constant w.r.t. the encoder parameters and can thus be ignored for training  
> Reconstruction Loss 
 
 $$ L= log\ p(x|z_q(x)) $$
### Reconstruction Loss : which optimizes the decoder and encoder
$$ z_q(x) = e_k, \quad where \quad k=argmin_j||z_e(x)-e_j||_2 \qquad (2)  $$  
- There is no real gradient defined for equation 2
- Approximate the gradient similar to the staright-through estimator
- Just copy gradients from decoder input $z_q(x)$ to encoder output $z_e(x)$

> Codebook Loss

$$ L = ||sg[z_e(x)]-e||_2^2 $$

### Codebook Loss : due to the fact that gradients bypass the embedding, we use a dictionary learning algorithm which uses an L2 error to move the embedding vectors $e_i$ towards the encoder output
- This loss term is only used for updating the dictionary

> Commitment Loss
$$ L = \beta||z_e(x)-sg[e]||_2^2 $$
### Commitment Loss : since the volume of the embedding space is dimensionless, it can grow arbirtarily if the embeddings $e_i$ do not train as fast as the encoder parameters, and thus we add a commitment loss to make sure that the encoder commits to an embedding
- Author found the resulting algorithm to be quite rebust to $\beta$, as the results did not vary for values of $\beta$ ranging from 0.1 to 2.0
- VQ-VAE use $\beta$ = 0.25 in all experiments  
** In general this would depend in the scale of reconstruction loss  

# Log-likelihood of the complete model

$$ log\ p(x) = log\ \sum_k d=p(x|z_k)p(z_k) $$
- Because the decoder $p(x|z)$ for $z = z_q(x)$ from MAP-inference
- the decoder should not allocate any probability mass to $p(x|z)$ for $z \ne z_q(x)$ once it has fully converged  
-> $ log\ p(x) \approx log\ p(x|z_q(x))p(z_q(x))$  
-> $ log\ p(x) \ge log\ p(x|z_q(x))p(z_q(x))$  (Jensen's inequality)

# Posterior Collapse
## Why the posterior collapse occurs
- When the decoder can sufficiently generate only past data without latent z
- The hypothesized Gaussian prior has no information in fact
- Gap between ELBO and evidence, failure of true posterior approximation
- Because the encoder does not express mearning z at the beginning of training

<img width="330" alt="VQVAE6" src="https://user-images.githubusercontent.com/60708119/200174355-d6ecbe62-aee0-47d2-8b8a-50a763d39a4d.png">
  
The approximate posterior mimics the prior as it is, and the model means that learning proceeds while ignoring the latent variable.


# PixelCNN Prior
After training VQ-VAE, train PixelCNN over the discrete latents for images  
Discrete latents vector generated by pre-trained PixelCNN enters the VQ-VAE decoder input and generate images