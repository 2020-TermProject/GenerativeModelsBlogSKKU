---
layout: post
title: "f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization"
date: 2022-11-13
author: Janghun Kim, Hyogeun Byun
categories: GAN
tags: [paper review, Neurips, GAN]
use_math: true
published: true
---

Nowozin, Sebastian, Botond Cseke, and Ryota Tomioka. "f-gan: Training generative neural samplers using variational divergence minimization." Advances in neural information processing systems 29 (2016).

# 1. Introduction
확률적 생성모델은 주어진 domain x상의 확률 분포를 서술한다. 가능한 모델 집합 Q가 주어졌을 때 다음에 관심이 있다.

- Sampling : Q로부터 sample을 생성한다. Sample을 살펴보거나 어떤 함숫값을 계산해봄으로써 우리는 분포에 대한 통찰을 얻거나 결정적인 문제를 해결 할 수 있다.
- Estimation : 잘 알려지지 않은 진짜 분포로부터 sample x가 주어졌을 때 이 진짜 분포를 잘 설명하는 Q < Q를 찾는다. 
- Point wise likelihood estimation : sample x가 주어지면 우도 Q(x)를 계산한다. 

GAN은 정확한 sampling과 근사추정이 가능한 인상적인 모델이다. 여기서 사용된 모델은 균등분포 같은 random input vector를 받는 feedforward 신경망이다.이런 확률적 feedforward 신경망을 generative neural samplers라고 부른다. original GAN에서, neural sample를 JSD의 근사적 최소화로 추정하는 것이 가능함이 증명되어 있다.

![image](/assets/fGAN_img/equation1.png)

GAN 학습의 중요한 테크닉은 동시에 최적화된 Discriminator 신경망을 만든 것에 있다. 이 논문에서는 GAN의 학습 목적을 임의의 f-divergence로 일반화하고자한다.

구체적으로, 이 논문에서 보여주는 state-of-the-art한 것은:
- GAN 학습목적을 모든 f-divergence에 대해 유도하고 여러 divergence 함수를 소개할 것이다: Kullback-Leibler와 Pearson Divergence를 포함한다.
- 우리는 GAN의 saddle-point 최적화를 단순화할 것이고 또 이론적으로 증명할 것이다.
- 자연 이미지에 대한 generative neural sampler을 측정하는 데 어느 divergence 함수가 적당한지 실험적 결과를 제시하겠다.

# 2. Method
### The f-divergence Family
KL Divergence는 두 확률 분포간의 차이를 측정한다. 두 분포 $P$와 $Q$가 있고, domain $x$에서 연속밀도함수 $p$와 $q$에 대해 $f$-divergence는 $f(1) = 0$이고 convex 하고 lower semicontinuous한 함수 $f$에 대해 아래과 같이 정의된다.

![image](/assets/fGAN_img/equation2.png)

### Variational Estimation of f-divergences
Nyugen 등 연구자는 $P$와 $Q$로부터의 sample만 주어진 경우에서 $f$-divergence를 측정하는 일반적인 변분법을 유도했다. 우리는 이를 고정된 모델에서 그 parameter를 측정하는 것으로까지 확장할 것이고, 이를 variational divergence minimization(VDM)이라 부를 것이다. 또한 적대적 생성 학습법은 이 VDM의 특수한 경우임을 보인다.

모든 볼록이고 하반연속인 볼록 켤레함수 $f∗$ (Fenchel conjugate)를 갖는다. 아래와 같이 정의된다.
![image](/assets/fGAN_img/equation3.png)

Nguyen 등 연구자는 lower bound를 다음과 같이 표현할 수 있으며,
![image](/assets/fGAN_img/equation4.png)

변분법을 취해서 다음과 같이 표현된다.
![image](/assets/fGAN_img/equation5.png)

아래는 여러 $f$-divergence를 생성함수와 함께 나타낸 것이다.
![image](/assets/fGAN_img/equation6.png)



![image](/assets/NADE_img/rbms.png)

Energy function을 이용하여 Probability를 구하지만, 다양한 문제가 존재한다. 
![image](/assets/NADE_img/rbms2.png)

- Computing partition function $Z는 소규모 network를 제외한 모든 network에서 intractable 하며 근사가 필요로 합니다.
- RMM은 probabilistic system의 일부를 modeling 하는 데 사용할 수 없습니다.
- 학습된 distribution을 평가하는데 어려움이 있습니다.

### Bayesian Networks
![image](/assets/NADE_img/bayesian.png)

다들 아시는 내용이지만, Bayesian Networks의 주요 strategy는 conditionals를 사용하여 distribution을 decomposing하는 것 입니다. 예를 들어, Fully Visible Sigmoid Belief Networks는 다음과 같습니다.

# 3. Neural Autoregressive Distribution Estimator (NADE)
![image](/assets/NADE_img/nade1.png)

NADE의 Architecture는 위와 같으며, 가중치 matrix $W를 공유하는 것을 확인할 수 있습니다. 그리고 next input에 대해서도 이전 Weight를 재활용하는 식의 strategy를 사용하여 computational cost가 절감되며, 빠르게 진행될 수 있다는 것을 알 수 있습니다.

![image](/assets/NADE_img/nade2.png)

NADE 네트워크에서 사용되는 파라미터를 계산한 결과인데 이는 weight matrix를 공유함으로써 파라미터 수가 상당히 적은 것을 확인할 수 있습니다.

NADE의 작동 방식은 다음과 같습니다.
- i 번째 픽셀을 1 ~ i-1 번째 픽셀에 의존하도록 conditionals를 사용합니다.
- 즉, 첫 번째 픽셀의 확률분포는 독립적으로 만들고, 두 번째 픽셀의 확률은 첫 번째 픽셀에 의존하도록 구성
- NADE는 explicit model (생성 + 확률 계산) -> 주어진 입력에 대해 density 계산 가능
- 연속 확률 변수일 때는 마지막 모델에 가우시안 믹스쳐 모델을 사용

![image](/assets/NADE_img/code.png)

# 4. Summary

### Uses the explicit representation of the joint distribution 

### Reduces the number of parameters by sharing weights in the neural network

### Generation is slow because the model generates one pixel at a time

### Possible to speed up the computation by reusing some previous computations

